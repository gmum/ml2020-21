{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"12_RNN_SOLVED.ipynb","provenance":[],"collapsed_sections":["laVdd5g5hAQu"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P95c6hK3hAQq"},"source":["# Rekurencyjne Sieci Neuronowe (RNN)"]},{"cell_type":"markdown","metadata":{"id":"laVdd5g5hAQu"},"source":["### Importy i Utilsy  (odpalić i schować )"]},{"cell_type":"code","metadata":{"id":"I0D3yk7lhAQu"},"source":["# imports \n","import torch\n","import os\n","import unicodedata\n","import string\n","import numpy as np\n","from typing import Tuple, Optional, List\n","\n","from torch.nn.functional import cross_entropy\n","\n","import matplotlib.pyplot as plt \n","from sklearn.metrics import f1_score\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","all_letters = string.ascii_letters\n","n_letters = len(all_letters)\n","\n","\n","class ListDataset(Dataset):\n","    \n","    def __init__(self, data, targets):\n","        \n","        self.data = data\n","        self.targets = targets\n","        \n","    def __getitem__(self, ind):\n","        \n","        return self.data[ind], self.targets[ind]\n","    \n","    def __len__(self):\n","        return len(self.targets)\n","\n","    \n","def unicode_to__ascii(s: str) -> str:\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n","                                                                 and c in all_letters)\n","                   \n","\n","def read_lines(filename: str) -> List[str]:\n","    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n","    return [unicode_to__ascii(line) for line in lines]\n","\n","\n","def letter_to_index(letter: str) -> int:\n","    return all_letters.find(letter)\n","\n","\n","def line_to_tensor(line: str) -> torch.Tensor:\n","    tensor = torch.zeros(len(line), n_letters)\n","    for i, letter in enumerate(line):\n","        tensor[i][letter_to_index(letter)] = 1\n","    return tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcSQvaMPhAQv"},"source":["## Dane sekwencyjne\n","\n","Modele, którymi zajmowaliśmy się wcześniej zakładały konkretny kształt danych. Dla przykładu klasyczna sieć neuronowa fully-connected zakładała, że na wejściu dostanie wektory rozmiaru 784 - dla wektorów o innej wymiarowości i innych obiektów model zwyczajnie nie będzie działać.\n","\n","Takie założenie bywa szczególnie niewygodne przy pracy z niektórymi typami danych, takimi jak:\n","* językiem naturalny (słowa czy zdania mają zadanej z góry liczby znaków)\n","* szeregi czasowe (dane giełdowe ciągną się właściwie w nieskończoność) \n","* dźwięk (nagrania mogą być krótsze lub dłuższe).\n","\n","Do rozwiązania tego problemu służą rekuencyjne sieci neuronowe (*recurrent neural networks, RNNs*), które zapamiętują swój stan z poprzedniej iteracji."]},{"cell_type":"markdown","metadata":{"id":"mH3chO87hAQv"},"source":["### Ładowanie danych\n","Poniższe dwie komórki ściągają dataset nazwisk z 18 różnych narodowości. Każda litera w danym nazwisku jest zamieniana na jej indeks z alfabetu w postaco kodowania \"one-hot\". Podsumowując, każde nazwisko jest binarną macierzą rozmiaru `n_letters` $\\times$ `len(name)`. \n","\n","Dodatkowo ponieważ ten dataset jest mocno niezbalansowany uzyjemy specjalnego samplera do losowania przykładów treningowych tak aby do uczenia sieć widziała tyle samo przykładów z każdej klasy.\n","\n","Ponieważ nazwiska mogą mieć różne długości będziemy rozważać `batch_size = 1` w tym notebooku (choć implementacje modeli powinny działać dla dowolnych wartości `batch_size`!)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maOHB6NZiRgr","executionInfo":{"status":"ok","timestamp":1610371885104,"user_tz":-60,"elapsed":1545,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"ddd780f4-74b9-4c06-e785-aaf5ff99e783"},"source":["!wget https://download.pytorch.org/tutorial/data.zip\n","!unzip data.zipq"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-01-11 13:31:24--  https://download.pytorch.org/tutorial/data.zip\n","Resolving download.pytorch.org (download.pytorch.org)... 13.32.204.34, 13.32.204.93, 13.32.204.49, ...\n","Connecting to download.pytorch.org (download.pytorch.org)|13.32.204.34|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2882130 (2.7M) [application/zip]\n","Saving to: ‘data.zip.2’\n","\n","\rdata.zip.2            0%[                    ]       0  --.-KB/s               \rdata.zip.2          100%[===================>]   2.75M  --.-KB/s    in 0.04s   \n","\n","2021-01-11 13:31:24 (64.3 MB/s) - ‘data.zip.2’ saved [2882130/2882130]\n","\n","unzip:  cannot find or open data.zipq, data.zipq.zip or data.zipq.ZIP.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DRGjkPZ2hAQv"},"source":["# NOTE: you can change the seed or remove it completely if you like\n","torch.manual_seed(1337)\n","\n","data_dir = 'data/names'\n","\n","# Build the category_lines dictionary, a list of names per language\n","category_lines = {}\n","all_categories = []\n","\n","data = []\n","targets = [] \n","label_to_idx = {}\n","\n","# read each natonality file and process data \n","for label, file_name in enumerate(os.listdir(data_dir)):\n","    \n","    label_to_idx[label] = file_name.split('.')[0].lower()\n","    \n","    names = read_lines(os.path.join(data_dir, file_name))\n","    data += [line_to_tensor(name) for name in names]\n","    targets += len(names) * [label]\n","\n","# split into train and test indices\n","test_frac = 0.1\n","n_test = int(test_frac * len(targets))\n","test_ind = np.random.choice(len(targets), size=n_test, replace=False)\n","train_ind = np.setdiff1d(np.arange(len(targets)), test_ind)\n","\n","targets = torch.tensor(targets)\n","train_targets = targets[train_ind]\n","\n","# calculate weights for BalancedSampler\n","uni, counts = np.unique(train_targets, return_counts=True)\n","weight_per_class = len(targets) / counts\n","weight = [weight_per_class[c] for c in train_targets]\n","# preapre the sampler\n","sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) \n","\n","train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n","train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n","\n","test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yvstu1-sldC6","executionInfo":{"status":"ok","timestamp":1610371886790,"user_tz":-60,"elapsed":3205,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"9862286e-15c1-44b5-ed41-a91bcc7ae593"},"source":["# check out the content of the dataset\n","for i, (x, y) in enumerate(train_loader):\n","    break\n","\n","print(\"x.shape:\", x.shape)\n","print(\"name: \", end=\"\")\n","for letter_onehot in x[0]:\n","    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n","\n","print(\"\\ny:\", label_to_idx[y.item()])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x.shape: torch.Size([1, 5, 52])\n","name: Abadi\n","y: arabic\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3VdtPOhhAQw"},"source":["## Zadanie 1. (2 pkt.)\n","\n","Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n","![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n","\n","* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n","* Wyjście z sieci możemy mieć dowolny rozmiar, potrzebna jest również warstwa przekształacjąca stan ukryty na wyjście.\n","* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n"]},{"cell_type":"code","metadata":{"id":"WNu0vccJhAQw"},"source":["class RNN(torch.nn.Module):\n","    \n","    def __init__(self, \n","                 input_size: int,\n","                 hidden_size: int, \n","                 output_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        :param output_size: int\n","            Desired dimensionality of the output vector\n","        \"\"\"\n","        super(RNN, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.input_to_hidden = torch.nn.Linear(in_features=input_size + hidden_size, \n","                                         out_features=hidden_size)  \n","        \n","        self.hidden_to_output = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n","    \n","    # for the sake of simplicity a single forward will process only a single timestamp \n","    def forward(self, \n","                input: torch.tensor, \n","                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n","        \"\"\"\n","        :param input: torch.tensor \n","            Input tesnor for a single observation at timestep t\n","            shape [batch_size, input_size]\n","        :param hidden: torch.tensor\n","            Representation of the memory of the RNN from previous timestep\n","            shape [batch_size, hidden_size]\n","        \"\"\"\n","        \n","        combined = torch.cat([input, hidden], dim=1) \n","        hidden = torch.tanh(self.input_to_hidden(combined)) # ???\n","        output =  self.hidden_to_output(hidden) # ???\n","        return output, hidden\n","    \n","    def init_hidden(self, batch_size: int) -> torch.Tensor:\n","        \"\"\"\n","        Returns initial value for the hidden state\n","        \"\"\"\n","        return torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIe3L-8LhAQw"},"source":["### Pętla uczenia"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"xXEsqqvxhAQx","executionInfo":{"status":"ok","timestamp":1610371940016,"user_tz":-60,"elapsed":56417,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"b9b9b88d-2753-4b5e-a013-9f6a1f9c7f04"},"source":["n_class = len(label_to_idx)\n","\n","# initialize network and optimizer\n","rnn = RNN(n_letters, 256, n_class).cuda()\n","optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n","\n","# we will train for only a single epoch \n","epochs = 1\n","\n","\n","# main loop\n","for epoch in range(epochs):\n","    \n","    loss_buffer = []\n","    \n","    for i, (x, y) in enumerate(train_loader):  \n","        \n","        x = x.cuda()\n","        y = y.cuda()\n","        \n","        optimizer.zero_grad()\n","        # get initial hidden state\n","        hidden = rnn.init_hidden(x.shape[0])\n","        \n","        # get output for the sample, remember that we treat it as a sequence\n","        # so you need to iterate over the 2nd, time dimensiotn\n","\n","        seq_len = x.shape[1]\n","        \n","        for t in range(seq_len): \n","            x_t = x[:, t]\n","            output, hidden = rnn(input=x_t, hidden=hidden)\n","            \n","        loss = cross_entropy(output, y)\n","        loss.backward()\n","        optimizer.step()  \n","        \n","        loss_buffer.append(loss.item())\n","        \n","        if i % 1000 == 1:\n","            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n","            loss_buffer = []\n","    \n","\n","# evaluate on the test set\n","with torch.no_grad():\n","    ps = []\n","    ys = []\n","    correct = 0\n","    for i, (x, y) in enumerate(test_loader):\n","        x = x.cuda()\n","        ys.append(y.numpy())\n","\n","        hidden = rnn.init_hidden(x.shape[0])\n","        seq_len = x.shape[1]\n","        for t in range(seq_len): \n","            x_t = x[:, t]\n","            output, hidden = rnn(input=x_t, hidden=hidden)\n","\n","        pred = output.argmax(dim=1)\n","        ps.append(pred.cpu().numpy())\n","    \n","    ps = np.concatenate(ps, axis=0)\n","    ys = np.concatenate(ys, axis=0)\n","    f1 = f1_score(ys, ps, average='weighted')\n","    \n","    print(f\"Final F1 score: {f1:.2f}\")\n","    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hiperparams!\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 Progress:  0% Loss: 2.935\n","Epoch: 0 Progress:  6% Loss: 2.860\n","Epoch: 0 Progress: 11% Loss: 2.766\n","Epoch: 0 Progress: 17% Loss: 2.495\n","Epoch: 0 Progress: 22% Loss: 2.258\n","Epoch: 0 Progress: 28% Loss: 2.135\n","Epoch: 0 Progress: 33% Loss: 2.013\n","Epoch: 0 Progress: 39% Loss: 1.916\n","Epoch: 0 Progress: 44% Loss: 1.865\n","Epoch: 0 Progress: 50% Loss: 1.783\n","Epoch: 0 Progress: 55% Loss: 1.847\n","Epoch: 0 Progress: 61% Loss: 1.778\n","Epoch: 0 Progress: 66% Loss: 1.673\n","Epoch: 0 Progress: 72% Loss: 1.694\n","Epoch: 0 Progress: 77% Loss: 1.672\n","Epoch: 0 Progress: 83% Loss: 1.607\n","Epoch: 0 Progress: 89% Loss: 1.611\n","Epoch: 0 Progress: 94% Loss: 1.623\n","Epoch: 0 Progress: 100% Loss: 1.673\n","Final F1 score: 0.17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sNeNU93qn7BC"},"source":["## Zadanie 2. (0.5 pkt.)\n","Zaimplementuj funkcje `predict`, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n","\n","**Hint**: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."]},{"cell_type":"code","metadata":{"id":"N8FhF_08hAQy"},"source":["def predict(name: str, rnn: RNN):\n","    x = line_to_tensor(name).unsqueeze(0).cuda()\n","\n","    hidden = rnn.init_hidden(x.shape[0])\n","    seq_len = x.shape[1]\n","    for t in range(seq_len): \n","        x_t = x[:, t]\n","        output, hidden = rnn(input=x_t, hidden=hidden)\n","\n","    res = output.topk(3, dim=1)\n"," \n","    for score, ind in zip(res.values[0], res.indices[0]):\n","        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4OWP8wqhAQy","executionInfo":{"status":"ok","timestamp":1610372364086,"user_tz":-60,"elapsed":485,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"26f522d9-02f5-4b8a-eaa0-3af40141d15f"},"source":["some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n","\n","for name in some_names:\n","    print(name)\n","    predict(name, rnn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Satoshi\n","\tjapanese: 3.69\n","\titalian: 3.40\n","\tarabic: 2.52\n","Jackson\n","\tscottish: 4.39\n","\trussian: 2.74\n","\tenglish: 2.72\n","Schmidhuber\n","\tgerman: 3.60\n","\tczech: 2.94\n","\tdutch: 2.84\n","Hinton\n","\tscottish: 2.68\n","\tenglish: 2.60\n","\tirish: 1.67\n","Kowalski\n","\tpolish: 6.46\n","\tczech: 3.91\n","\trussian: 3.10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nNETvP06hAQz"},"source":["## Zadanie 3 (4 pkt.)\n","Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n","\n","![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n","\n","* W klasie `LSTMCell` ma znaleźć się główna loginka LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n","* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przenisiemy to do klasy modelu.\n","* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n","\n","Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n","\n","Zadaniem jest osiągnięcie wartości $f1_score$ lepszej niż na sieci RNN, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmiana `random seed`.\n","\n","#### Komórka LSTM"]},{"cell_type":"code","metadata":{"id":"GNKRxYwChAQz"},"source":["class LSTMCell(torch.nn.Module):\n","\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        \"\"\"\n","        \n","        super(LSTMCell, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # initialize LSTM weights \n","        # NOTE: there are different approaches that are all correct \n","        # (e.g. single matrix for all input opperations), you can pick\n","        # whichever you like for this task\n","    \n","        self.W_f = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_i = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_c = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_o = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","\n","    def forward(self, \n","                input: torch.tensor, \n","                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n","        \n","        hidden, cell = states\n","        \n","        # Compute input, forget, and output gates\n","        # then compute new cell state and hidden state\n","        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n","        \n","        combined = torch.cat([input, hidden], dim=1)\n","        \n","        forget_gate = torch.sigmoid(self.W_f(combined))\n","        input_gate = torch.sigmoid(self.W_i(combined))\n","        \n","        cell_update = torch.sigmoid(self.W_c(combined))\n","        cell = forget_gate * cell + input_gate * cell_update\n","        \n","        output = torch.sigmoid(self.W_o(combined))\n","        hidden = output * torch.tanh(cell)\n","        \n","        return hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5U5U8kizhAQz"},"source":["### Klasa modelu LSTM"]},{"cell_type":"code","metadata":{"id":"G2MyIu3_hAQz"},"source":["class LSTM(torch.nn.Module):\n","\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        \"\"\"\n","        \n","        super(LSTM, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n","        \n","    def forward(self, \n","                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n","        \"\"\"\n","        :param input: torch.tensor \n","            Input tesnor for a single observation at timestep t\n","            shape [batch_size, input_size]\n","        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n","        \"\"\"\n","        \n","        batch_size = input.shape[0]\n","        \n","        hidden, cell = self.init_hidden_cell(batch_size)\n","        \n","        hiddens = []\n","        cells = []\n","        \n","        # this time we will process the whole sequence in the forward method\n","        # as oppose to the previous exercise, remember to loop over the timesteps\n","        \n","        time_steps = input.shape[1]\n","        \n","        for t in range(time_steps):\n","            x = input[:, t]\n","            hidden, cell = self.cell(x, (hidden, cell))\n","            \n","            hiddens.append(hidden)\n","            cells.append(cell)\n","\n","        return torch.stack(hiddens), torch.stack(cells)\n","    \n","    def init_hidden_cell(self, batch_size):\n","        \"\"\"\n","        Returns initial value for the hidden and cell states\n","        \"\"\"\n","        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda(), \n","                torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qRxPI-nhAQz"},"source":["### Pętla uczenia"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LVCWqsVhAQ0","executionInfo":{"status":"ok","timestamp":1610372306534,"user_tz":-60,"elapsed":147972,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"70088f64-dd7c-4713-bcf0-4d43e6fd9bd8"},"source":["from itertools import chain\n","\n","# torch.manual_seed(1337)\n","\n","# build data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n","test_loader = DataLoader(test_dataset, batch_size=1)\n","\n","# initialize the lstm with an additional cliassifier layer at the top\n","lstm = LSTM(input_size=len(all_letters), hidden_size=128).cuda()\n","clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx)).cuda()\n","\n","# initialize a optimizer\n","params = chain(lstm.parameters(), clf.parameters())\n","optimizer = torch.optim.Adam(params, lr=0.01) \n","\n","# we will train for only a single epoch \n","epoch = 1\n","\n","# main loop\n","for epoch in range(epoch):\n","    \n","    loss_buffer = []\n","    \n","    for i, (x, y) in enumerate(train_loader):   \n","        \n","        x = x.cuda()\n","        y = y.cuda()\n","        \n","        optimizer.zero_grad()\n","        \n","        # get output for the sample, remember that we treat it as a sequence\n","        # so you need to iterate over the sequence length here\n","        \n","        hidden, state = lstm(x)\n","        output = clf(hidden[-1])\n","        # calucate the loss\n","        loss = cross_entropy(output, y)\n","        loss.backward()\n","        optimizer.step()                                \n","        \n","        loss_buffer.append(loss.item())\n","        \n","        if i % 1000 == 1:\n","            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n","            loss_buffer = []\n","\n","# evaluate on the test set\n","with torch.no_grad():\n","    \n","    ps = []\n","    ys = []\n","    for i, (x, y) in enumerate(test_loader): \n","        \n","        x = x.cuda()\n","        ys.append(y.numpy())\n","        \n","        hidden, state = lstm(x)\n","        output = clf(hidden[-1])\n","\n","        pred = output.argmax(dim=1)\n","        ps.append(pred.cpu().numpy())\n","    \n","    ps = np.concatenate(ps, axis=0)\n","    ys = np.concatenate(ys, axis=0)\n","    f1 = f1_score(ys, ps, average='weighted')\n","    \n","    print(f\"Final F1 score: {f1:.2f}\")\n","    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 Progress:  0% Loss: 1.986\n","Epoch: 0 Progress:  6% Loss: 2.691\n","Epoch: 0 Progress: 11% Loss: 2.015\n","Epoch: 0 Progress: 17% Loss: 1.836\n","Epoch: 0 Progress: 22% Loss: 1.636\n","Epoch: 0 Progress: 28% Loss: 1.516\n","Epoch: 0 Progress: 33% Loss: 1.389\n","Epoch: 0 Progress: 39% Loss: 1.345\n","Epoch: 0 Progress: 44% Loss: 1.240\n","Epoch: 0 Progress: 50% Loss: 1.250\n","Epoch: 0 Progress: 55% Loss: 1.193\n","Epoch: 0 Progress: 61% Loss: 1.104\n","Epoch: 0 Progress: 66% Loss: 1.040\n","Epoch: 0 Progress: 72% Loss: 1.122\n","Epoch: 0 Progress: 77% Loss: 0.911\n","Epoch: 0 Progress: 83% Loss: 1.007\n","Epoch: 0 Progress: 89% Loss: 0.957\n","Epoch: 0 Progress: 94% Loss: 0.997\n","Epoch: 0 Progress: 100% Loss: 0.910\n","Final F1 score: 0.21\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gGXUhgroo7AN"},"source":["## Zadanie 4. (0.5 pkt.)\n","Zaimplementuj analogiczną do funkcji `predict` z zadania 2 dla modelu `lstm+clf`.\n"]},{"cell_type":"code","metadata":{"id":"-ChJv1fphAQ0"},"source":["def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n","    x = line_to_tensor(name).unsqueeze(0).cuda()\n","\n","    hidden, state = lstm(input=x)\n","    output = clf(hidden[-1])\n","\n","    res = output.topk(3, dim=1)\n"," \n","    for score, ind in zip(res.values[0], res.indices[0]):\n","        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgQcGWqthAQ0","executionInfo":{"status":"ok","timestamp":1610372379028,"user_tz":-60,"elapsed":631,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"outputId":"ce0d95db-27e4-46bf-cb89-49ad0a954aff"},"source":["# test your lstm predictor\n","some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n","    \n","for name in some_names:\n","    print(name)\n","    predict_lstm(name, lstm, clf)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Satoshi\n","\tarabic: 0.03\n","\tjapanese: -1.19\n","\trussian: -3.46\n","Jackson\n","\tscottish: 0.09\n","\tenglish: -1.60\n","\trussian: -3.67\n","Schmidhuber\n","\tgerman: 0.49\n","\trussian: -1.44\n","\tenglish: -2.81\n","Hinton\n","\tfrench: -0.04\n","\tenglish: -0.21\n","\tirish: -0.96\n","Kowalski\n","\tpolish: 2.81\n","\tczech: -4.55\n","\trussian: -4.67\n"],"name":"stdout"}]}]}